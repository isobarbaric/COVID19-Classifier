{
    "abstract": [
        {
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract",
            "text": "The Covid-19 outbreak, which emerged in 2020, became the top priority of the world. The fight against this disease, which has caused millions of people's deaths, is still ongoing, and it is expected that these studies will continue for years. In this study, we propose an improved learning model to predict the severity of the patients by exploiting a combination of machine learning techniques. The proposed model uses an adaptive boost algorithm with a decision tree estimator and a new parameter tuning process. The learning ratio of the new model is promising after many repeated experiments are performed by using different parameters to reduce the effect of selecting random parameters. The proposed algorithm is compared with other recent state-of-the-art algorithms on UCI data sets and a recent Covid-19 dataset. It is observed that competitive accuracy results are obtained, and we hope that this study unveils more usage of advanced machine learning approaches."
        }
    ],
    "body_text": [
        {
            "cite_spans": [
                {
                    "end": 288,
                    "ref_id": "BIBREF25",
                    "start": 278,
                    "text": "(Lu, 2019;"
                },
                {
                    "end": 326,
                    "ref_id": "BIBREF36",
                    "start": 289,
                    "text": "Shhadat, Hayajneh, & Al-Sharif, 2020)"
                }
            ],
            "ref_spans": [],
            "section": "Introduction",
            "text": "Humankind has endeavoured to make sense of the events around it since the first years of its existence. Machine learning (ML) algorithms have helped much understand and predict unknown events. Predictions have gained dimensions and approaches for more complex and numerous data (Lu, 2019; Shhadat, Hayajneh, & Al-Sharif, 2020) . A group of input variables evaluates the predicted variable (dependent variable in statistics). With these variables, many tools such as trees, different regression techniques, nearest neighbours, boosting algorithms, etc., have been studied to put forward a model that maps the input variables to output classes. However, studies do not provide a desired level of accuracy most of the time. This study originates from such a need and requirement. It tries to propose a model which achieves a mapping close to optimum in a short period."
        },
        {
            "cite_spans": [
                {
                    "end": 142,
                    "ref_id": "BIBREF33",
                    "start": 122,
                    "text": "(Seni & Elder, 2010)"
                }
            ],
            "ref_spans": [],
            "section": "Introduction",
            "text": "Usage of ML algorithms increases the computing power and accuracy, especially ensemble methods come forward at this point (Seni & Elder, 2010) . The ensemble methods enable us to train multiple models using the same learning algorithm. The proposed algorithm of this study is an excellent example of this technique. Besides that, a model having an integrated ensemble algorithm is more likely to get the minimum error and higher accuracy levels."
        },
        {
            "cite_spans": [
                {
                    "end": 90,
                    "ref_id": "BIBREF11",
                    "start": 76,
                    "text": "(Freund, 1995)"
                },
                {
                    "end": 133,
                    "ref_id": "BIBREF12",
                    "start": 108,
                    "text": "(Freund & Schapire, 1997)"
                },
                {
                    "end": 461,
                    "ref_id": "BIBREF32",
                    "start": 445,
                    "text": "(Schapire, 2003;"
                },
                {
                    "end": 483,
                    "ref_id": "BIBREF37",
                    "start": 462,
                    "text": "Souza & Matwin, 2012)"
                }
            ],
            "ref_spans": [],
            "section": "Introduction",
            "text": "Freund and Shapire introduced the Adaptive Boosting (AdaBoost) algorithm in (Freund, 1995) then improved it (Freund & Schapire, 1997) . In the algorithm, they used so-called \"weak\" classifiers to perform better performance which can lead the AdaBoost into a powerful, high-performance algorithm. They developed an exponential loss function to update the weights. After that, many studies have been made and presented, and some of them are as in (Schapire, 2003; Souza & Matwin, 2012) ."
        },
        {
            "cite_spans": [
                {
                    "end": 536,
                    "ref_id": "BIBREF26",
                    "start": 515,
                    "text": "(Lu, Hu, & Bai, 2015)"
                }
            ],
            "ref_spans": [],
            "section": "Introduction",
            "text": "The AdaBoost is a popular classification algorithm. During the training phase, the distribution weight of the sample is increased as the error rate increases, and oppositely as it decreases, the new distribution weight is reduced. Then samples are continually trained with the unknown distribution weights. The aim is to have strong feedback by reducing the next machine's error and reaching better accuracy rates in the end. The process of the AdaBoost algorithm can be found easily, and one sample research is in (Lu, Hu, & Bai, 2015) ."
        },
        {
            "cite_spans": [
                {
                    "end": 781,
                    "ref_id": "BIBREF23",
                    "start": 745,
                    "text": "(Lai, Shih, Ko, Tang, & Hsueh, 2020)"
                }
            ],
            "ref_spans": [],
            "section": "Introduction",
            "text": "Almost one and a half year ago, the World Health Organization (WHO) stated to the world that COVID-19 is a pandemic. The rapid spread of the disease around the globe brought it necessary to take many measures immediately. Unfortunately, the final figures are embarrassing since over 178 million people are infected (confirmed) and over 3.8 million dead all over the world according to dashboard) by the end of March 2021. The main goal of this study is to support and contribute the solution to an epidemic. The virus spreads mainly through saliva droplets. After being infected, the disease causes pneumonia in the lungs, which cause difficulty in breathing. The most common symptoms in COVID-19 patients were recorded as fever and cough as in (Lai, Shih, Ko, Tang, & Hsueh, 2020) ."
        },
        {
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction",
            "text": "The proposed model of the study is an empowered ensemble method using an adaptive boost strategy. The algorithm gets \"Decision Tree\" as the base classifier. In a standard AdaBoost model, predictions are made on the training set, and the weight of input is updated due to the error rate. Then a second classifier is trained using the updated weight, and it makes predictions on the training set again. This feedback results in a remarkable decrease in error. This procedure goes on in this way up to the end."
        },
        {
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction",
            "text": "Briefly, a machine-learning algorithm has been proposed and tuned for quick and effective learning here. This tuning process is done for empowering the primary model using an adaptive boosting approach. One of the best capabilities of the proposed algorithm is that it obtains and gets the results in a moderate computer in short periods due to other known AI and ML algorithms. The way the proposed algorithm works is that it drops into filtering methods since it is related to the correlations between the input and the output classes."
        },
        {
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction",
            "text": "The experiments are conducted on two datasets. In the first part, well-known UCI-datasets (Uci machine learning repository: data sets) are used, and even this part is also divided into two sections. In the first section, ensemble algorithms are shown using two sample datasets. This section shows the reason for selecting the adaptive boosting approach. Then in the second section of the first part, the proposed algorithm is compared with known sample ML datasets taken from (Datasets: Feature selection @ asu; Uci machine learning repository: data sets). This is done for presenting the accuracy rates of all compared algorithms using these datasets. Finally, in the second part, the proposed algorithm is compared with other known and recent algorithms to present the aim of this study. This paper is organized as follows. Section 2 presents the recent studies on the topic. Section 3 shows the proposed algorithm after brief information on AdaBoost Algorithm and Decision Trees. Then Section 4 presents the results of this study in three consecutive subsections. In the first one, comparisons of all ensemble methods, in the second one comparisons of known and recent algorithms, and the third one comparisons of other state-of-the-art algorithms are compared. Finally, Section 5 presents conclusions and future works of the study."
        },
        {
            "cite_spans": [],
            "ref_spans": [],
            "section": "Related work",
            "text": "Although ML algorithms have major sub-fields, supervised learning studies are the most known and popular ones. In this method, algorithms are mainly categorized into two main types,filter and wrapper algorithms. The main difference between these two algorithms is that filtering algorithms try to evaluate the correlation with the output class. In contrast, wrapper methods try to find out a subset of input variables to use. There are also hybrid approaches as well in the literature."
        },
        {
            "cite_spans": [
                {
                    "end": 250,
                    "ref_id": "BIBREF34",
                    "start": 237,
                    "text": "(Sevinc, 2019"
                },
                {
                    "end": 277,
                    "ref_id": "BIBREF35",
                    "start": 251,
                    "text": "& Sevin\u00e7 & D\u00f6keroglu, 2019"
                },
                {
                    "end": 380,
                    "ref_id": "BIBREF17",
                    "start": 346,
                    "text": "(Huang, Zhou, Ding, & Zhang, 2012)"
                },
                {
                    "end": 483,
                    "ref_id": "BIBREF19",
                    "start": 467,
                    "text": "(Karakaya, 2017)"
                },
                {
                    "end": 631,
                    "ref_id": "BIBREF7",
                    "start": 602,
                    "text": "(Deniz, Kiziloz, Dokeroglu, &"
                },
                {
                    "end": 647,
                    "ref_id": "BIBREF7",
                    "start": 632,
                    "text": "Cosar, 2017 and"
                },
                {
                    "end": 661,
                    "ref_id": "BIBREF34",
                    "start": 648,
                    "text": "Sevinc, 2019)"
                },
                {
                    "end": 929,
                    "ref_id": "BIBREF42",
                    "start": 911,
                    "text": "(Xue et al., 2019)"
                },
                {
                    "end": 1296,
                    "ref_id": "BIBREF43",
                    "start": 1272,
                    "text": "(Zhu, Ong, & Dash, 2007)"
                }
            ],
            "ref_spans": [],
            "section": "Related work",
            "text": "Feature selection methods, most of which have been used and studied mainly for analysis, classification, categorization, pattern detection, etc., are popular in supervised learning. Some recent studies on classification are presented in (Sevinc, 2019 & Sevin\u00e7 & D\u00f6keroglu, 2019 . Feature selection techniques are done by extreme learning machine (Huang, Zhou, Ding, & Zhang, 2012) and improved by GA methodologies for better performance. A similar GA is presented in (Karakaya, 2017) . The aim is to find a generic solution in a reasonable amount of time after optimizing an improved GA. Similarly, in (Deniz, Kiziloz, Dokeroglu, & Cosar, 2017 and Sevinc, 2019) , some filtering mechanisms and methodologies supported by extreme learning machines have been developed and experimented with for feature subset selection. These studies are good examples of filter-based feature selection approach while Xue et al. (Xue et al., 2019) presents a wrapper feature selection algorithm for classification. They proposed a reweighted multi-view algorithm which allowed multiple relevant views for better accuracy. As a hybrid approach, Zhu et al. proposed a hybrid filter and wrapper feature selection algorithm with a combination of genetic algorithm (GA) and local search (LS) in (Zhu, Ong, & Dash, 2007) ."
        },
        {
            "cite_spans": [
                {
                    "end": 444,
                    "ref_id": "BIBREF10",
                    "start": 397,
                    "text": "(Dokeroglu, Sevinc, Kucukyilmaz, & Cosar, 2019)"
                },
                {
                    "end": 502,
                    "ref_id": "BIBREF27",
                    "start": 486,
                    "text": "(Mirjalili, 2015"
                },
                {
                    "end": 520,
                    "ref_id": "BIBREF24",
                    "start": 503,
                    "text": "& Li et al., 2020"
                }
            ],
            "ref_spans": [],
            "section": "Related work",
            "text": "Additionally, there are other algorithms, such as meta-heuristic algorithms, in the literature. Artificial Bee Colony, Bacterial Foraging, Bat Algorithm, Gary wolf, whale optimization, etc., are involved in this group. These methodologies aim to reach better classification performances by upgrading some parameters and implementing specific feature selection techniques. A survey is presented in (Dokeroglu, Sevinc, Kucukyilmaz, & Cosar, 2019) and two additional study is presented in (Mirjalili, 2015 & Li et al., 2020 . The final two studies from which this study benefited are binary classification algorithms that affected and developed from the movements of dragonflies."
        },
        {
            "cite_spans": [],
            "ref_spans": [],
            "section": "Related work",
            "text": "If it comes to the ML algorithm for classification, it is a vast field and still developing. You can find many ML algorithms for regression and classification problems. The most popular ML website is presented in (Api reference) in which python base classes and utility functions are listed. One can easily create unique models by using integrated most common ML algorithms and get remarkable results."
        },
        {
            "cite_spans": [
                {
                    "end": 92,
                    "ref_id": "BIBREF37",
                    "start": 70,
                    "text": "(Souza & Matwin, 2012)"
                },
                {
                    "end": 543,
                    "ref_id": "BIBREF4",
                    "start": 508,
                    "text": "(Bai, Xie, Wang, Zhang, & Li, 2021)"
                }
            ],
            "ref_spans": [],
            "section": "Related work",
            "text": "As a research study on ML algorithms, a prominent one can be found in (Souza & Matwin, 2012) . It is one of the initial examples of the Adaptive Boost (AdaBoost) algorithm. The authors used resampling with substitution instead of the re-weighting approach for reaching a good \"weak learner\". They claimed that the proposed model produced the slightest error for specific weight distribution by this process. Because the weaker learner you get, the lower error rate you reach. In another interesting study in (Bai, Xie, Wang, Zhang, & Li, 2021) , a similar AdaBoost algorithm is put forward which is close to the approach in here. Their model has three elements mainly and the learning performance of the model is reinforced by the AdaBoost approach. Finally, it is claimed that the proposed model overwhelms all the comparison models in terms of root-mean-square error, threshold statistics, and residuals analysis. It is also added that they successfully improve the sensitivity and regression capacity of the model by handling the rough knowledge synchronously with their AdaBoost methodology."
        },
        {
            "cite_spans": [
                {
                    "end": 151,
                    "ref_id": "BIBREF22",
                    "start": 136,
                    "text": "(Kiziloz, 2021)"
                },
                {
                    "end": 631,
                    "ref_id": "BIBREF29",
                    "start": 593,
                    "text": "(Priore, Ponte, Puente, & G\u00f3mez, 2018)"
                },
                {
                    "end": 1012,
                    "ref_id": "BIBREF0",
                    "start": 991,
                    "text": "(Albadr et al., 2020)"
                }
            ],
            "ref_spans": [],
            "section": "Related work",
            "text": "Among ML algorithms, Ensemble algorithms must be taken into consideration since they have an essential role in learning methodology. In (Kiziloz, 2021) , five popular ensemble methods are implemented for feature selection, and models are run on known datasets from (Uci machine learning repository: data sets). When used as the only algorithm in the model, it has been reported that it executes fast; however, when they are used together and integrated, they perform better in terms of accuracy. This is believed to be reasonable, and this methodology is also performed in this study. Also in (Priore, Ponte, Puente, & G\u00f3mez, 2018) , a scheduling based using ensemble methods of machine learning algorithms has been presented. They deal with the scheduling problem and try to improve the result by considering the recommendations made by different ensemble methods of ML algorithms. Thus, they try to obtain a conceptual evolution in the design of control systems. In addition, the study in (Albadr et al., 2020) tries to identify the effectiveness of detecting COVID-19 using chest X-ray images by integrating Extreme Learning Machine with genetic algorithm evolutionary capabilities."
        },
        {
            "cite_spans": [
                {
                    "end": 204,
                    "ref_id": "BIBREF41",
                    "start": 169,
                    "text": "(Wang, Liao, Li, Yan, & Chen, 2021)"
                },
                {
                    "end": 663,
                    "ref_id": "BIBREF13",
                    "start": 623,
                    "text": "(Guz, Cuendet, Hakkani-Tur, & Tur, 2010;"
                },
                {
                    "end": 708,
                    "ref_id": "BIBREF18",
                    "start": 664,
                    "text": "Jabri, Saidallah, Alaoui, & Fergougui, 2018;"
                },
                {
                    "end": 747,
                    "ref_id": "BIBREF21",
                    "start": 709,
                    "text": "Khan, Ahamed, Kadry, & Ramasamy, 2020)"
                }
            ],
            "ref_spans": [],
            "section": "Related work",
            "text": "Ensemble methods are so attractive that you can encounter easily as in a typical combinatorial optimization problem, a Dynamic Vehicle Routing Problem (DVRP) studied in (Wang, Liao, Li, Yan, & Chen, 2021) . The authors try to find a solution to the problem with time windows constraint. Finally, they construct a multi-objective optimization model for the problem and improve it by an ensemble learning method. They claim that the ensemble learning model can help to improve the capability of adapting to new environments with different dynamic conditions. Similar to these studies, there are many other studies such as in (Guz, Cuendet, Hakkani-Tur, & Tur, 2010; Jabri, Saidallah, Alaoui, & Fergougui, 2018; Khan, Ahamed, Kadry, & Ramasamy, 2020) . All these studies aim to show the power of AdaBoost algorithm, especially in binary classification problems. After tuning and making repeated experiments on a real dataset, i.e. Covid-19, we also try to present the success of the proposed model against best known and recent state-of-the-art algorithms."
        },
        {
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proposed algorithm",
            "text": "In this part, a tuned Adaptive Boost (AdaBoost) Algorithm is presented. A model that is empowered by a classifier is a candidate solution to predict and solve such regression and classification problems with higher success."
        },
        {
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proposed algorithm",
            "text": "Another aim of the algorithm is to reach the possible best accuracy within a short period. Execution times will also be mentioned here since it is an essential issue for any machine learning algorithm. The results of the experiments are the averages of 10 independent runs of program executions to get rid of any randomness effect."
        },
        {
            "cite_spans": [
                {
                    "end": 75,
                    "ref_id": "BIBREF12",
                    "start": 50,
                    "text": "(Freund & Schapire, 1997)"
                }
            ],
            "ref_spans": [],
            "section": "Proposed algorithm",
            "text": "Adaptive Boosting (AdaBoost), first introduced by (Freund & Schapire, 1997) is briefly a general ensemble method that creates a strong classifier by using a number of weak classifiers. The algorithm starts with a weak learner, weighting each example equally. This is obtained by applying weights w 1 , w 2 , \u2026w N to each training sample, which is called boosting iterations. At the beginning, all the weights are equally set to w i = 1 N . Then a weak learner on the original data is trained by training dataset. At each iteration, sample weights are updated, and this continues as it is reapplied to the data again up to the end. At each step, as the correct predictions are made, the weight of the training example is decreased, and oppositely the weight is increased if the model incorrectly predicted it. In other words, misclassified examples get their weights increased for the next round(s), while correctly classified ones get their weights decreased. Finally, predictions are integrated with a weighted majority sum to get the final prediction."
        },
        {
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proposed algorithm",
            "text": "Machine learning models generally suffer from bias or variances, and many studies are proposed to minimize this effect. Ensemble learning methods are good examples to mention related to the point. These methodologies are known to make training and predictions based on different models. Then by combining them, these models tend to have less bias and be less data-sensitive. The two most popular ensemble methods are bagging and boosting."
        },
        {
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proposed algorithm",
            "text": "\u2022 Bagging: Its name comes from Bootstrap AGGregatINGand mostly applies the decision tree method as a Bootstrap variance. Bagging trains all individual models parallel and each model is trained by a random subset of the dataset. As a result, the average of all the predictions from different trees are evaluated and unbiased when compared to a single decision tree \u2022 Boosting: This method trains a group of individual models in a sequential way. Each individual model gets a feedback from mistakes made by the previous model. Subsequent trees are re-trained at every step since the goal is to solve the absolute error caused by the previous tree. This method shows how AdaBoost works mainly."
        },
        {
            "cite_spans": [],
            "ref_spans": [
                {
                    "end": 60,
                    "ref_id": "FIGREF0",
                    "start": 54,
                    "text": "Fig. 1"
                }
            ],
            "section": "Proposed algorithm",
            "text": "A sample illustration of these methods can be seen in Fig. 1 . As mentioned, the proposed algorithm uses Boosting strategy and it can be seen that a high learning rate is more probable."
        },
        {
            "cite_spans": [],
            "ref_spans": [],
            "section": "Adaptive boost algorithm",
            "text": "Though multi label classification is possible, because of Covid-19 data set, a binary classification will be shown. Each data is presented and labeled as in Eq. 1"
        },
        {
            "cite_spans": [],
            "ref_spans": [],
            "section": "Adaptive boost algorithm",
            "text": "where k is training data set size, x i \u2208 T and y i \u2208{-1, 1}. T represents the training data and the set {-1, 1} binary class labels for the data elements. "
        },
        {
            "cite_spans": [],
            "ref_spans": [],
            "section": "E. Sevin\u00e7",
            "text": "Each training data sample x i can be a point in a multidimensional feature space. AdaBoost by itself implements a probability distribution over all the training samples. This distribution is modified by iterations with a new weak classifier to the data. In this study, the probability distribution is denoted as D t (x i ) and t refers to the successive iterations of the algorithm. The weak classifier chosen for the iteration t is denoted by h t . Then the class label assigned by x i is presented by h t (x i ). By comparing h t (x i ) with y i for i = 1, 2, \u2026,k, there will be an error \u220a t , which is called as the classification error rate for the classifier h t ."
        },
        {
            "cite_spans": [],
            "ref_spans": [],
            "section": "E. Sevin\u00e7",
            "text": "Each candidate classifier is trained during iterations using a subsampling of all of the training data as provided by the probability distribution D t (x). As a result, the higher the probability D t (x) for a given training sample x, the greater the chance that it will be chosen for training the candidate classifier h(t). The selection of h t is essential since among all different possible values, the one that minimizes the misclassification rate \u220a t must be chosen. For example, a weak classifier is primarily a single feature that is simply a threshold in most AdaBoost implementations."
        },
        {
            "cite_spans": [],
            "ref_spans": [],
            "section": "E. Sevin\u00e7",
            "text": "The other important point for AdaBoost algorithm is the trust level \u03b1 t , of weak classifier in which we trust. As mentioned clearly before, the bigger the value of error, \u220a t for a classifier, the lower the trust must be."
        },
        {
            "cite_spans": [],
            "ref_spans": [],
            "section": "E. Sevin\u00e7",
            "text": "The relation between \u03b1 t and \u220a t is shown in Eq. 2"
        },
        {
            "cite_spans": [
                {
                    "end": 390,
                    "ref_id": "BIBREF5",
                    "start": 361,
                    "text": "(Cao, Miao, Liu, & Gao, 2014)"
                }
            ],
            "ref_spans": [],
            "section": "E. Sevin\u00e7",
            "text": "If the error rate, \u220a t , gets closer to 1 starting from 0, the trust level of the candidate classifier h(t) will get a value in the scale of \u2212 \u221e and \u221e. \u220a t being close to 1 means that the weak classifier fails almost completely on the overall training dataset, and \u220a t being close to 0 means that your weak classifier is a powerful classifier as also stated in (Cao, Miao, Liu, & Gao, 2014) A final classifier H is obtained at the end. After k iterations, H classifier in AdaBoost algorithm is evaluated as in Eq. 3"
        },
        {
            "cite_spans": [
                {
                    "end": 518,
                    "ref_id": "BIBREF32",
                    "start": 502,
                    "text": "(Schapire, 2003)"
                },
                {
                    "end": 588,
                    "ref_id": null,
                    "start": 573,
                    "text": "(Api reference)"
                }
            ],
            "ref_spans": [],
            "section": "E. Sevin\u00e7",
            "text": "where x is the new data element which denotes the information strength in the training data. For example, in binary classification if H(x) is positive, then the predicted class is 1, otherwise, it is \u2212 1. The classifier presents a weighted aggregation produced by the individual weak classifiers. Thus, in the end, a single strong, weak classifier can dominate over several not-so-strong weak classifiers with the highest trust factor \u03b1 t among others. A fundamental AdaBoost algorithm is explained in (Schapire, 2003) , and related python libraries can be easily found in (Api reference) ."
        },
        {
            "cite_spans": [],
            "ref_spans": [],
            "section": "E. Sevin\u00e7",
            "text": "The proposed AdaBoost algorithm of this study works through the following steps;"
        },
        {
            "cite_spans": [],
            "ref_spans": [],
            "section": "E. Sevin\u00e7",
            "text": "1. Initially, Adaboost creates and assigns training and test subsets randomly. 2. It trains the model through iterations by selecting the training set. 3. It assigns the higher weight to wrong classified observations so that in the next iteration, these observations will get a high probability for classification. 4. Algorithm assigns the weight to the classifier after each iteration due to the tuned Decision Tree classifier. 5. The whole process continues until the complete training data fits without any error or reaches a maximum number of estimators. 6. To classify, perform a \"vote\" among classifiers and decide the output due to the model built."
        },
        {
            "cite_spans": [],
            "ref_spans": [],
            "section": "Decision trees",
            "text": "Decision trees are well-known and popular for classification problems. Their popularity mainly originates from the similarity to that of standard human brain's making decisions. Being a supervised machine learning algorithm, a decision tree includes a series of sequential decisions given for reaching a specific result. This mechanism is adopted for reaching better accuracy in the tests. The decision tree classifier evaluates due to the majority over many decisions and then makes the decision."
        },
        {
            "cite_spans": [
                {
                    "end": 436,
                    "ref_id": "BIBREF30",
                    "start": 410,
                    "text": "(Raileanu & Stoffel, 2004)"
                }
            ],
            "ref_spans": [],
            "section": "Decision trees",
            "text": "The decision tree algorithm starts with attribute election. For this, Attribute Selection Measure (ASM) split records and provides a rank and weight for the dataset features. Gini index and information gain are the most popular methods for ASM. According to these methods, features take place at the root node or the internal nodes; then, evaluation is done. More information about the indexes can be found in (Raileanu & Stoffel, 2004) ."
        },
        {
            "cite_spans": [],
            "ref_spans": [],
            "section": "Decision trees",
            "text": "Different decision tree algorithms use gini, entropy, or a combination of both. Gini index and entropy are the criteria for evaluating information gain. Decision tree algorithms use this value to split a node. These two measures show the noise/impurity degree of a node. For example, if a node has multiple classes, it indicates impurity, and oppositely if a node has only one class, it means it is pure. Finally, a decision tree is a graph-based solution to a problem representing all possible solutions to a trial by decisions based on given conditions."
        },
        {
            "cite_spans": [],
            "ref_spans": [],
            "section": "Tuning AdaBoost with decision tree classifier",
            "text": "The proposed algorithm here is called Empowered ADAboost with Decision Tree (E-ADAD) method. The algorithm mainly uses tuning parameters of related python libraries defined in the \"sklearn\" package. Python libraries have a wide range of capabilities by simply making function calls to manage and achieve a wide range of abilities. Many types of different models can be developed and used for other purposes for future problems.\"Decision Tree\" classifier has been used as the estimator and the \"gini\" method has been implemented by this classifier. In a binary classification problem such as the Covid-19 dataset, gini criterion is more popular and powerful. Its formula is presented in Eq. 4."
        },
        {
            "cite_spans": [],
            "ref_spans": [],
            "section": "Tuning AdaBoost with decision tree classifier",
            "text": "where p(c i ) is the probability of class c i in that node. Gini Index suggests a two-way split for the attributes and thus we can compute a weighted sum of the impurity for each partition separately. The classifier of E-ADAD model definition is given below; clf = AdaBoostClassifier( DecisionTreeClassifier(criterion = 'gini', max depth = 4), learning rate = 0.9)"
        },
        {
            "cite_spans": [],
            "ref_spans": [],
            "section": "Tuning AdaBoost with decision tree classifier",
            "text": "Though it is not shown, there is a split strategy in the constructor, namely \"best\". It is assigned by default. Additionally, another important parameter is the \"maximum depth of the tree\". A higher maximum depth value causes overfitting, while a lower one causes underfitting. This value is assigned as \"4\" as seen in the model definition."
        },
        {
            "cite_spans": [],
            "ref_spans": [],
            "section": "Tuning AdaBoost with decision tree classifier",
            "text": "E-ADAD uses the \"learning rate\" parameter, which is denoted generally as \u03b1, shows the speed of learning that the model achieves."
        },
        {
            "cite_spans": [],
            "ref_spans": [],
            "section": "Tuning AdaBoost with decision tree classifier",
            "text": "However, slower learning brings forward another significant point. A low learning rate will take much time and more probability to converge or get stuck in an undesirable local minimum. However, a higher one makes the learning jump over minima. In this study, 0.8 \u2a7d\u03b1\u2a7d1.0 is selected for E-ADAD since this range was observed to be more appropriate during the tests. Another point is that different \u03b1 values being in the range are taken for different datasets."
        },
        {
            "cite_spans": [],
            "ref_spans": [],
            "section": "Tuning AdaBoost with decision tree classifier",
            "text": "Number of estimators is another parameter that affects accuracy. The default value is 50, and this number is the point where boosting is terminated. The default value is chosen and used for E-ADAD."
        },
        {
            "cite_spans": [
                {
                    "end": 134,
                    "ref_id": "BIBREF15",
                    "start": 101,
                    "text": "(Hastie, Rosset, Zhu, & Zou, 2009"
                }
            ],
            "ref_spans": [],
            "section": "Tuning AdaBoost with decision tree classifier",
            "text": "Finally, algorithm{ \u2032 SAMME.R \u2032 } is assigned by default. It is also clearly emphasized in the study (Hastie, Rosset, Zhu, & Zou, 2009 ) that the twoclass AdaBoost builds an additive model to approximate the two-class Bayes rule. The SAMME.R is a natural and clean multi-class extension of the two-class AdaBoost algorithm. In other words, by its nature SAMME/SAMME.R adapts to the philosophy of boosting, and it is strongly believed that it is a powerful method in two-class predictions."
        },
        {
            "cite_spans": [],
            "ref_spans": [],
            "section": "Tuning AdaBoost with decision tree classifier",
            "text": "E-ADAD algorithm is presented below;"
        },
        {
            "cite_spans": [],
            "ref_spans": [],
            "section": "Algorithm 1. E-ADAD Algorithm",
            "text": "Due to result of AdaBoost algorithm, results are obtained on the test dataset and according to supervised learning methodology, success rate is evaluated."
        },
        {
            "cite_spans": [],
            "ref_spans": [],
            "section": "Comparison among other ensemble algorithms",
            "text": "The experiments are executed on a standard laptop Windows-10 machine. It has an i7 CPU (i7-5700HQ CPU @ 2.70 GHz) and 16 GB of memory. The code is implemented in Jupyter-notebook Version 3.0.0. All coding is in Python 3.6 version and *.ipynb format."
        },
        {
            "cite_spans": [],
            "ref_spans": [],
            "section": "Comparison among other ensemble algorithms",
            "text": "Ensemble algorithms are well-known among ML algorithms. They have an essential role since they are flexible and integrated by different types of estimators. These ensemble estimators are ideal for regression and classification problems since they are capable of reducing bias and variance to increase the performance of models."
        },
        {
            "cite_spans": [
                {
                    "end": 308,
                    "ref_id": "BIBREF22",
                    "start": 293,
                    "text": "(Kiziloz, 2021)"
                }
            ],
            "ref_spans": [],
            "section": "Comparison among other ensemble algorithms",
            "text": "Decision trees are the ones that are mostly integrated with ensemble algorithms. They are easy to understand and produce remarkable results in a world full of algorithms that look like a black box. Even after combining several models, ensemble methods improve learning much, as also stated in (Kiziloz, 2021) ."
        },
        {
            "cite_spans": [],
            "ref_spans": [],
            "section": "Comparison among other ensemble algorithms",
            "text": "Adaptive Boost (Adaboost) Algorithm: AdaBoost is powerful to arrange a set of weak classifiers in a sequence in which each weak classifier is the best choice for a classifier at that point for rectifying the errors made by the previous classifier. Boosting means arranging a set of weak classifiers in a sequence in which each weak classifier is the best choice for a classifier to rectify the errors made by the previous classifier."
        },
        {
            "cite_spans": [],
            "ref_spans": [],
            "section": "Comparison among other ensemble algorithms",
            "text": "AdaBoost can improve its ability to learn from past errors since it has the theoretical guarantee that as more weak classifiers are put into use, the final misclassification rate must be made arbitrarily small. Additionally, the AdaBoost approach has a bound on the generalization error. This means it will not ever increase since the updated weight will decrease. Finally, this method constitutes the origin of this study."
        },
        {
            "cite_spans": [],
            "ref_spans": [],
            "section": "Comparison among other ensemble algorithms",
            "text": "Random Forest Algorithm: Random Forest is a tree-based machine learning algorithm that integrates the solving capability of multiple decision trees. As the name suggests, this approach proposes a \"forest\" of trees. Decision trees are randomly created, and each node in the tree works on a random subset of features to predict the output. Then this random forest collects and integrates the result of individual decision trees, then evaluates the final output."
        },
        {
            "cite_spans": [],
            "ref_spans": [],
            "section": "Comparison among other ensemble algorithms",
            "text": "In the experiments, data sets have been divided into train and test subsets with different proportions obtained by randomly selected same subsets. This is done for getting rid of the effect of any time randomness. In other words, random_state parameter must be set to a value in order not to get different results after each execution. This is important because when we use random state = some number parameter, this means that each split will be random but the same every time. It provides us to work on the same split as the first time it is assigned."
        },
        {
            "cite_spans": [],
            "ref_spans": [],
            "section": "Comparison among other ensemble algorithms",
            "text": "A sample code using sci-kit for splitting the dataset is given below; from sklearn.model.selection import train test split X train, X test, y train, y test = train test split(X, y, test size = 0.25, random state = 0) According to that code, the training size is 75% of data while the test size is 25%. The split is first randomly done and is the same for each run. That means the initial random selection is the same for each run/split. random state = 0 means that the data will be randomly split, but the same random parts will be selected. The number \"0\" has no impact on this."
        },
        {
            "cite_spans": [],
            "ref_spans": [],
            "section": "Comparison among other ensemble algorithms",
            "text": "Histogram-based Gradient Boosting: Histogram-based Gradient Boosting approach is very similar to that of Gradient Boosting except for the compatibility with the dataset. This classification tree estimator is relatively faster than GradientBoostingClassifier for big datasets (number of samples \u2a7e10000)."
        },
        {
            "cite_spans": [],
            "ref_spans": [],
            "section": "Comparison among other ensemble algorithms",
            "text": "In this algorithm, each predictor is to be improved by reducing the errors due to its predecessor. However, Gradient Boosting fits a new predictor of the previous predictor's residual errors instead of providing a predictor on the data at each iteration."
        },
        {
            "cite_spans": [],
            "ref_spans": [
                {
                    "end": 97,
                    "ref_id": "FIGREF1",
                    "start": 91,
                    "text": "Fig. 2"
                },
                {
                    "end": 530,
                    "ref_id": "FIGREF1",
                    "start": 524,
                    "text": "Fig. 2"
                }
            ],
            "section": "Comparison among other ensemble algorithms",
            "text": "Finally, ensemble algorithms are compared to show the best one. The results can be seen in Fig. 2 . For all the algorithms, \"Iris\" and \"Wisconsin Breast Cancer\" datasets have been used. These datasets are mostly used ones while \"Iris\" is multi-class classification and \"Wisconsin Breast Cancer\" is a two-class classification problem. The reason for selecting these two common datasets is to show a clear distinction among the candidate algorithms. All ensemble classification algorithms are experienced on these datasets in Fig. 2 . Though all the results are close to each other AdaBoost algorithm is slightly more successful when compared to others. Esp. in a binary class classification problem, AdaBoost Algorithms are more successful, as seen in \"Wisconsin Breast Cancer\". It can be observed that it is about 5% better than the average of the rest algorithms."
        },
        {
            "cite_spans": [
                {
                    "end": 35,
                    "ref_id": "BIBREF39",
                    "start": 13,
                    "text": "(Too & Mirjalili, 2021"
                }
            ],
            "ref_spans": [],
            "section": "Comparison with state-of-the-art algorithms",
            "text": "The study in (Too & Mirjalili, 2021 ) is a very recent and remarkable one in the literature. Binary Dragonfly Algorithm is discussed and used, then some parameters are changed, and a new algorithm, hyper learning binary dragonfly algorithm (HLBDA), is proposed. This new algorithm, HLBDA, has been compared with eight other state-of-the-art algorithms, and the results are presented."
        },
        {
            "cite_spans": [
                {
                    "end": 182,
                    "ref_id": "BIBREF39",
                    "start": 159,
                    "text": "(Too & Mirjalili, 2021)"
                },
                {
                    "end": 343,
                    "ref_id": "BIBREF28",
                    "start": 326,
                    "text": "(Mirjalili, 2016)"
                },
                {
                    "end": 411,
                    "ref_id": "BIBREF16",
                    "start": 382,
                    "text": "(He, Xie, Wong, & Wang, 2018)"
                },
                {
                    "end": 554,
                    "ref_id": "BIBREF20",
                    "start": 528,
                    "text": "(Kennedy & Eberhart, 1997)"
                },
                {
                    "end": 626,
                    "ref_id": "BIBREF31",
                    "start": 594,
                    "text": "(Sayed, Hassanien, & Azar, 2019)"
                },
                {
                    "end": 740,
                    "ref_id": "BIBREF8",
                    "start": 677,
                    "text": "Souza, de Macedo, dos Santos Coelho, Pierezan, & Mariani, 2020)"
                },
                {
                    "end": 825,
                    "ref_id": "BIBREF14",
                    "start": 804,
                    "text": "(Hansen & Kern, 2004)"
                },
                {
                    "end": 958,
                    "ref_id": "BIBREF38",
                    "start": 933,
                    "text": "(Tanabe & Fukunaga, 2014)"
                }
            ],
            "ref_spans": [
                {
                    "end": 968,
                    "ref_id": "TABREF0",
                    "start": 961,
                    "text": "Table 1"
                }
            ],
            "section": "Comparison with state-of-the-art algorithms",
            "text": "Intuitively, this study is also affected by that study and proposes another ML algorithm. Then we make another comparison with all the algorithms presented in (Too & Mirjalili, 2021) from which a total of 9 algorithms is imported. The other mentioned eight well-known algorithms in there are; Binary Dragonfly Algorithm (BDA) (Mirjalili, 2016) , binary artificial bee colony (BABC) (He, Xie, Wong, & Wang, 2018) , binary multiverse optimizer (BMVO) (Al-Madi, Faris, & Mirjalili, 2019), binary particle swarm optimization (BPSO) (Kennedy & Eberhart, 1997) , chaotic crow search algorithm (CCSA) (Sayed, Hassanien, & Azar, 2019) , binary coyote optimization algorithm (BCOA) (de Souza, de Macedo, dos Santos Coelho, Pierezan, & Mariani, 2020) , evolution strategy with covariance matrix adaptation (CMAES) (Hansen & Kern, 2004) , and success-history based adaptive differential evolution with linear population size reduction (LSHADE) (Tanabe & Fukunaga, 2014) . Table 1 presents the parameters that are used by these algorithms."
        },
        {
            "cite_spans": [
                {
                    "end": 663,
                    "ref_id": "BIBREF39",
                    "start": 640,
                    "text": "(Too & Mirjalili, 2021)"
                }
            ],
            "ref_spans": [
                {
                    "end": 710,
                    "ref_id": "TABREF1",
                    "start": 703,
                    "text": "Table 2"
                }
            ],
            "section": "Comparison with state-of-the-art algorithms",
            "text": "All the datasets are also included here for making a more fair comparison. These datasets from (Datasets: Feature selection @ asu; Uci machine learning repository: data sets) are popular and commonly known in the ML area. The result is that the same algorithms using the same datasets are used and compared here. However, \"Horse Colic\" is an exception since it has 30% of the values are missing, and there is no clue how it had been used, and there is no given mapping plan for those values. Because of that reason, this dataset, i.e. \"Horse Colic\", has been excluded from the study. As a result, 20 out of 21 datasets have been used as in (Too & Mirjalili, 2021) . The dataset definitions are given in Table 2 . Some of them have medium while others have a big size."
        },
        {
            "cite_spans": [],
            "ref_spans": [],
            "section": "Comparison with state-of-the-art algorithms",
            "text": "However, it will be beneficial to give the following information about the datasets used in the experiments. In the \"Hepatitis\" dataset of (Uci machine learning repository: data sets), a lot of missing values are found. 155 instances have 20 features. These missing values are completed as stated in the explanation part of the dataset. Since all the values can be completed with the stated average values, no row or column dropped."
        },
        {
            "cite_spans": [],
            "ref_spans": [],
            "section": "Comparison with state-of-the-art algorithms",
            "text": "For \"Primary Tumor\", only 5 columns have missing values. These are completed as in the explanation part of the dataset."
        },
        {
            "cite_spans": [],
            "ref_spans": [],
            "section": "Comparison with state-of-the-art algorithms",
            "text": "\"Soybean\" dataset has the most problematic missing values. It is a medium-sized dataset having 307 rows \u00d7 35 columns. There are 19 output classes of soybean, all of which have small differences due to their classification. All values have been completed and used due to definitions."
        },
        {
            "cite_spans": [],
            "ref_spans": [],
            "section": "Comparison with state-of-the-art algorithms",
            "text": "\"Arrhythmia\" is also a medium-level dataset having 279 features in total. There are a total of 5 columns that have missing values. According to this study, if the number of missing values is greater than 10% of the whole, that column is not taken, namely dropped. This is the case for 13 th column. Since that column has more than 83% missing values, it has been drooped. For the rest four columns (10,11,12, and 14), they are within limits, and the completion process is done due to the column's mode."
        },
        {
            "cite_spans": [],
            "ref_spans": [],
            "section": "Comparison with state-of-the-art algorithms",
            "text": "\"Dermatology\" data set has are only missing value in one column, and only eight rows have missing values. They are again in limits, so those values are filled with the mean of that column."
        },
        {
            "cite_spans": [],
            "ref_spans": [],
            "section": "Comparison with state-of-the-art algorithms",
            "text": "For \"Glass\", \"Lymphography\", \"Ionosphere\", \"Zoo\", \"Musk-1\", \"SPECT Heart\", \"Libras Movement\", \"ILPD\", \"Seeds\", \"LSVT\", \"SCADI\", \"TOX_171\", \"Leukemia\", \"Lung discrete\" and \"Colon\" datasets, no missing values have been detected. So all the values of datasets are used and included in the experiments."
        },
        {
            "cite_spans": [
                {
                    "end": 213,
                    "ref_id": "BIBREF39",
                    "start": 190,
                    "text": "(Too & Mirjalili, 2021)"
                }
            ],
            "ref_spans": [
                {
                    "end": 65,
                    "ref_id": null,
                    "start": 59,
                    "text": "Fig. 3"
                }
            ],
            "section": "Comparison with state-of-the-art algorithms",
            "text": "Predicted accuracy levels of all algorithms can be seen in Fig. 3 . E-ADAD has a better performance among most of the datasets (12 in 20) when compared to all of the algorithms presented in (Too & Mirjalili, 2021) ."
        },
        {
            "cite_spans": [],
            "ref_spans": [],
            "section": "Comparison with state-of-the-art algorithms",
            "text": "In a typical learning prediction model, all data has been divided into three parts, training, validation, and test subsets. This discrimination has some drawbacks, such as test set data in the model and evaluation of the test can hardly be seen on general performance. Grouping data in this way may cause some misplacement. This effect can also be called leakage of data. In addition to this factor, the number of samples for learning or testing can drastically reduce caused after partitioning them. These effects totally can result in worsening the performance of the model."
        },
        {
            "cite_spans": [],
            "ref_spans": [],
            "section": "Comparison with state-of-the-art algorithms",
            "text": "As a solution to this problem, Cross-Validation (CV) has been used in the experiments. CV is in fact a critical technique for avoiding a methodological mistake at the same time. This means that there will be no unseen data up to that time that the model used. In this method, a cross-validation splitting strategy must be determined initially. Then the generator divides or splits the whole data into k equal parts. By default, this k value is 5 in sci-kit library. It means you implement 5-fold crossvalidation with the generator."
        },
        {
            "cite_spans": [],
            "ref_spans": [
                {
                    "end": 327,
                    "ref_id": "FIGREF2",
                    "start": 321,
                    "text": "Fig. 4"
                }
            ],
            "section": "Comparison with state-of-the-art algorithms",
            "text": "After splitting the whole data into k \"folds\", the model is trained using (k-1) of the folds as training data, and with the last fold, it will be validated. This whole process will be done repeatedly until k times to equally finish all the splits created by the generator in turn. An illustration of k-fold CV is seen in Fig. 4 ."
        },
        {
            "cite_spans": [],
            "ref_spans": [
                {
                    "end": 101,
                    "ref_id": "TABREF2",
                    "start": 87,
                    "text": "Tables 3 and 4"
                }
            ],
            "section": "Comparison with state-of-the-art algorithms",
            "text": "Numbers related to the \"Learning Curves\" and \"Fitness Times\" of each fold are given in Tables 3 and 4 respectively. The performance of CV is calculated by taking the average of the values of splits computed in turn and all the figures are put together for a clear comparison of the datasets."
        },
        {
            "cite_spans": [],
            "ref_spans": [],
            "section": "Comparison with state-of-the-art algorithms",
            "text": "Then the performance of CV is calculated by taking the average of the values of splits computed in turn. Though this approach seems to need a high computation time, it gives the advantage of getting rid of randomness data leakage. Additionally, you can get rid of the effect of a small number of training or test datasets simultaneously."
        },
        {
            "cite_spans": [],
            "ref_spans": [
                {
                    "end": 94,
                    "ref_id": "FIGREF3",
                    "start": 88,
                    "text": "Fig. 5"
                },
                {
                    "end": 143,
                    "ref_id": "FIGREF4",
                    "start": 137,
                    "text": "Fig. 6"
                }
            ],
            "section": "Comparison with state-of-the-art algorithms",
            "text": "If to see all the methodologies together, \"Learning Curves\" of Datasets is presented in Fig. 5 , while \"Fitness Times\" of datasets is in Fig. 6 ."
        },
        {
            "cite_spans": [],
            "ref_spans": [
                {
                    "end": 9,
                    "ref_id": "FIGREF3",
                    "start": 3,
                    "text": "Fig. 5"
                }
            ],
            "section": "Comparison with state-of-the-art algorithms",
            "text": "In Fig. 5 , learning curve of a Decision Tree classifier is shown as the training set score and the cross-validation scores of the prediction together. This graph is in support of showing accuracy and learning speeds at the same time. These two figures, namely Figs. 5 and 6 are obtained by cross-validator and the whole dataset has been divided by ShuffleSplit function of the sci-kit library. After splitting data into training and test sets, which are 75% and 25% respectively in the study, then the results are automatically evaluated and the results are produced."
        },
        {
            "cite_spans": [],
            "ref_spans": [
                {
                    "end": 236,
                    "ref_id": "FIGREF4",
                    "start": 230,
                    "text": "Fig. 6"
                }
            ],
            "section": "Comparison with state-of-the-art algorithms",
            "text": "High learning curves can easily be noticed. It shows the appropriateness of the model. Then accordingly, a high cross-validation rates is following them. The results of these CV rates are the results of E-ADAD classifier. Then in Fig. 6 , the model shows the time how the CV results has been achieved in seconds. It seems extremely fast when compared to its rivals."
        },
        {
            "cite_spans": [],
            "ref_spans": [],
            "section": "Comparison with state-of-the-art algorithms",
            "text": "Finally, the proposed algorithm has a very fast convergence speed and learning rate, even in the early iterations as clearly seen in Figs. 5 and 6."
        },
        {
            "cite_spans": [],
            "ref_spans": [],
            "section": "Real dataset comparison",
            "text": "2020 began with a disaster, and the World Health Organization (WHO) announced that Covid-19 became an epidemic. Unfortunately, initial cases were met in China, and the virus rapidly spread over the world. Still, we have many restrictions in our daily life."
        },
        {
            "cite_spans": [
                {
                    "end": 185,
                    "ref_id": "BIBREF39",
                    "start": 162,
                    "text": "(Too & Mirjalili, 2021)"
                }
            ],
            "ref_spans": [],
            "section": "Real dataset comparison",
            "text": "In this section, the proposed algorithm is used to solve a real-life problem, namely a Covid-19 patient health prediction dataset. This dataset is the same as in (Too & Mirjalili, 2021) , and it can be found on (Atharva-Peshkar) on the Internet."
        },
        {
            "cite_spans": [],
            "ref_spans": [
                {
                    "end": 50,
                    "ref_id": "TABREF4",
                    "start": 43,
                    "text": "Table 5"
                }
            ],
            "section": "Real dataset comparison",
            "text": "The description of the dataset is shown in Table 5 . In the dataset, some symptoms are given and accordingly, the death and recovery conditions are given related to given factors due to those 15 features."
        },
        {
            "cite_spans": [],
            "ref_spans": [],
            "section": "Real dataset comparison",
            "text": "In the whole dataset, there are 1081 instances totally. 1018 instances out of 1081 have the value \"0\" which denotes alive and the rest 63 instances have the value \"1\" which denotes dead. In the whole dataset, there are totally 1081 instances. 1018 instances out of 1081 have the value \"0\" which denotes alive and the rest 63 instances have the value \"1\" which denotes dead. There are totally 2 classes in the whole dataset. 75% and 25% were used in the study for training and testing sets respectively."
        },
        {
            "cite_spans": [
                {
                    "end": 90,
                    "ref_id": "BIBREF39",
                    "start": 67,
                    "text": "(Too & Mirjalili, 2021)"
                }
            ],
            "ref_spans": [
                {
                    "end": 765,
                    "ref_id": "FIGREF5",
                    "start": 752,
                    "text": "Figs. 7 and 8"
                },
                {
                    "end": 859,
                    "ref_id": "FIGREF5",
                    "start": 853,
                    "text": "Fig. 7"
                },
                {
                    "end": 1044,
                    "ref_id": "FIGREF6",
                    "start": 1038,
                    "text": "Fig. 8"
                }
            ],
            "section": "Real dataset comparison",
            "text": "This study is intended to make a comparison with the algorithms in (Too & Mirjalili, 2021) since it is the most recent, state-of-the-art, and published ML algorithm. The proposed algorithm of that study is HLBDA, and additional four other algorithms (HLBDA, BDA, BMVO, and BPSO) have been compared, which are stated there. The parameters pl and gl of HLBDA are declared to be set to 0.7 and 0.85, respectively, and success rates are given in the study. It is told that after updating these two values as stated, HLBDA had a fast convergence speed, which means it gets closer to the final result very quickly. Then it is also claimed that HLBDA had a better success learning rate due to the other mentioned four algorithms. The results are presented in Figs. 7 and 8 . The results of model performance due to the training set and the model is as seen in Fig. 7 . Though the proposed algorithm in that study is told to be converged very quickly, E-ADAD trains all training set in 0.14 s and it reaches a better accuracy rate in the period. Fig. 8 shows the accuracy rates of all algorithms. E-ADAD has got a 95.33% accuracy rate, and this value is the mean of 10 independent runs. It overcomes HLBDA with 92.21% prediction accuracy being the highest among others in that study."
        },
        {
            "cite_spans": [],
            "ref_spans": [],
            "section": "Real dataset comparison",
            "text": "E-ADAD has the best prediction rate even in a short period. With the help of the \"Decision Tree\" classifier, E-ADAD has overcome all the other recent and state-of-the-art algorithms."
        },
        {
            "cite_spans": [],
            "ref_spans": [
                {
                    "end": 526,
                    "ref_id": "FIGREF7",
                    "start": 520,
                    "text": "Fig. 9"
                }
            ],
            "section": "Real dataset comparison",
            "text": "Moreover, two more methods are mentioned for emphasizing the capability of the E-ADAD algorithm. The first one is confusion matrix commonly used for summarizing the performance of a classification algorithm. It is specifically used for the cases if there is a big difference between the classes. A binary data set is used here and in the test dataset, there are 271 instances which are 25% of 1081 total instances. 253 of these are declared as \"0\", meaning NOT death, and 18 of these are \"1\" in the test part as seen in Fig. 9 . Anyway, proportions are being preserved as they are on the whole. Receiver Operating Characteristic (ROC) curve is also another parameter to evaluate classifier output quality. One characteristic of ROC curves is that the \"true positive rate\"s are placed on the Y axis, while \"false positive rate\"s are on the X-axis. In other words, the top left corner of the plot is the \"ideal\" point -a false positive rate of zero, and a true positive rate of one. However, this point is an extreme, but the closer the better. It implies that a larger Area Under the Curve (AUC) is simply preferable."
        },
        {
            "cite_spans": [],
            "ref_spans": [
                {
                    "end": 324,
                    "ref_id": "FIGREF0",
                    "start": 317,
                    "text": "Fig. 10"
                }
            ],
            "section": "Real dataset comparison",
            "text": "The AUC in the proposed algorithm is evaluated as 0.722 and the blue line shows no sign of classification capability or random selectivity while the red line shows the E-ADAD's performance. This implies a reasonable and remarkable classification has been achieved between the classes of the binary dataset as seen in Fig. 10 . "
        },
        {
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion and future work",
            "text": "In this study, an Adaptive Boost Algorithm using a Decision Tree estimator is proposed. After making trials, a tuning process is done to a classifier, and a binary classification problem is solved."
        },
        {
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion and future work",
            "text": "AdaBoost is popular and well-known technique in this field."
        },
        {
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion and future work",
            "text": "Moreover, it is improved for binary and multi-class classifications. It is seen that the proposed learning model substantially can achieve a good performance both in execution and the average prediction rates. Even for binary classification problems, the algorithm produces way better results than state-of-the-art algorithms. This study is also believed to be used and implemented for other real-life situations during epidemic conditions. Additionally, for different data and conditions, the E-ADAD algorithm seems to be a good candidate to be benefited. Among its rivals, it seems to predict and achieve in short periods. This is thought to be an outstanding capability and speciality. As a result, E-ADAD achieves the highest accuracy for predicting output classes."
        },
        {
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion and future work",
            "text": "For future studies, E-ADAD can be tuned to get more successful results with multi-class classifications. There is a wide range of research in this field. Especially for regression problems, E-ADAD needs to be searched much for getting far better results. There are many ML fields in new models, and classifiers are proposed and put forward for better results."
        },
        {
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion and future work",
            "text": "Additionally, ensemble algorithms are very promising for proposing new models. Probable newly proposed models with different classifiers can be very beneficial while solving other types of real-life problems. An ensemble algorithm integrated can be useful for especially unsupervised learning and unlabeled data. "
        }
    ],
    "metadata": {
        "paper_id": "2797954bd1e3706c018b17f41bf5249f9431dc7b",
        "title": "An empowered AdaBoost algorithm implementation: A COVID-19 dataset study"
    }
}