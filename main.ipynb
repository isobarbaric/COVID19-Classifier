{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "scientific_titles = []\n",
    "with open('json/related-articles_pro-science.json', 'r') as storage:\n",
    "    info = json.loads(storage.read())\n",
    "    for article in info:\n",
    "        scientific_titles.append(article[0])\n",
    "\n",
    "conspiracy_titles = [] \n",
    "with open('json/related-articles_conspiracy.json', 'r') as storage:\n",
    "    info = json.loads(storage.read())\n",
    "    for article in info:\n",
    "        conspiracy_titles.append(article[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from dateutil.parser import parse\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import string\n",
    "import nltk\n",
    "from sqlalchemy import false \n",
    "\n",
    "class BagOfWords:\n",
    "    def __init__(self, tokenized_paragraph: list, is_positive: bool):\n",
    "        self.sentences = tokenized_paragraph\n",
    "        self.is_positive = is_positive\n",
    "\n",
    "    def tokenize(self):\n",
    "        self.words = []\n",
    "        for sentence in self.sentences:\n",
    "            for word in word_tokenize(sentence):\n",
    "                self.words.append(word)\n",
    "\n",
    "    def to_lower_case(self):\n",
    "        for i in range(len(self.words)):\n",
    "            self.words[i] = self.words[i].lower()\n",
    "\n",
    "    def clean_data(self):\n",
    "        noise = ['...', \"n't\"]\n",
    "        def is_time_or_date(word):  \n",
    "            try:\n",
    "                parsed = parse(word)\n",
    "                return True\n",
    "            except:\n",
    "                return False\n",
    "\n",
    "        # rn = set()\n",
    "\n",
    "        for i in range(len(self.words)-1, -1, -1):\n",
    "            if len(self.words[i]) <= 2 or self.words[i].isnumeric() or is_time_or_date(self.words[i]) or self.words[i] in noise:\n",
    "                self.words.pop(i)\n",
    "                continue\n",
    "        \n",
    "            # shave punctation off of beginnings and from the end\n",
    "            start_ind, end_ind = -1, -1\n",
    "            for j in range(len(self.words[i])):\n",
    "                if self.words[i][j] in string.ascii_lowercase or self.words[i][j].isnumeric():\n",
    "                    start_ind = j\n",
    "                    break\n",
    "            for j in range(len(self.words[i])-1, -1, -1):\n",
    "                if self.words[i][j] in string.ascii_lowercase or self.words[i][j].isnumeric():\n",
    "                    end_ind = j\n",
    "                    break\n",
    "\n",
    "            if (start_ind == 0 and end_ind == len(self.words[i])-1) or start_ind >= end_ind:\n",
    "                continue\n",
    "\n",
    "            self.words[i][j]\n",
    "            # rn.add((self.words[i], self.words[i][start_ind:end_ind+1]))\n",
    "\n",
    "    def remove_stop_words(self):\n",
    "        for i in range(len(self.words)-1, -1, -1):\n",
    "            if self.words[i] in stopwords.words('english'):\n",
    "                self.words.pop(i)  \n",
    "\n",
    "    def normalize_words(self):\n",
    "        def get_part_of_speech(provided_word):\n",
    "            _, part_of_speech = nltk.pos_tag([provided_word])[0]\n",
    "            if 'NN' in part_of_speech:\n",
    "                return 'n'\n",
    "            if 'VB' in part_of_speech:\n",
    "                return 'v'\n",
    "            if 'JJ' in part_of_speech:\n",
    "                return 'a'\n",
    "            if 'RB' in part_of_speech:\n",
    "                return 'r'\n",
    "            return 'n'\n",
    "\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        for i in range(len(self.words)):\n",
    "            self.words[i] = lemmatizer.lemmatize(self.words[i], get_part_of_speech(self.words[i]))\n",
    "\n",
    "    def create_frequency_chart(self):\n",
    "        self.freqChart = dict()\n",
    "        for word in self.words:\n",
    "            if word not in self.freqChart.keys():\n",
    "                self.freqChart[word] = 1\n",
    "            else:\n",
    "                self.freqChart[word] += 1\n",
    "\n",
    "        # sorting in ascending order by value\n",
    "        self.freqChart = {i: self.freqChart[i] for i in sorted(self.freqChart, key=self.freqChart.get, reverse=True)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(\"'star-killer\", 'star-killer'), (\"'ve\", 've'), (\"'virtual\", 'virtual'), ('long-term…', 'long-term'), ('u.s.', 'u.s'), ('cpt®', 'cpt'), (\"'viral\", 'viral'), ('lgbtq+', 'lgbtq'), (\"'re\", 're')}\n"
     ]
    }
   ],
   "source": [
    "a = BagOfWords(scientific_titles, True)\n",
    "\n",
    "a.tokenize()\n",
    "a.to_lower_case()\n",
    "a.clean_data()\n",
    "a.remove_stop_words()\n",
    "\n",
    "# todo: improve part of speech performance\n",
    "# a.normalize_words()\n",
    "# print(a.words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c633c67666e88adde68d379e2922af970c32b82fe9487bc44a79690061d5013b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
