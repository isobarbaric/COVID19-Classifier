plan for next week:
- split up data into testing, training set in an 80-20 split
- after this, evaluation of well the system works with a ML model will be good
    - train model and redo-word frequency part once for the testing set and once for the training set (to prevent data leakage)
    - generate table for training and testing data (2 tables)
        - columns: for each of the 20 keywords
        - rows: count for each keyword in an article  
        - add a final column to the right displaying whether the article was a conspiracy or scientific (0 = conspiracy, 1 = scientific) 
    - train model with various different methods and evaluate test set  

concerns:
- certain articles that are longer are skewing # occurences of a word 
    - Jackson is best example
    - large number of 0s in all of the columns
- data cleaning is acting weird
    - letter 'm' has a lot of frequency somehow
    - need to remove unicode chars (\c2019) and escape characters - although doesn't seem to be making much of a difference
- scaling - to use or not 
    - try with different scalers and different models