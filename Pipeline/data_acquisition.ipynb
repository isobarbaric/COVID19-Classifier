{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Scraping <a href='https://mediabiasfactcheck.com'>MediaBiasFactCheck</a> to collect Pro-Science and Conspiracy-PseudoScience Articles</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the required libraries\n",
    "\n",
    "import requests\n",
    "import os\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining variables to facilitate scraping\n",
    "\n",
    "suffixes = ['.com', '.org', '.edu', '.gov', '.int', '.co', '.net', '.au', '.us', '.uk', '.ne', 'news']\n",
    "\n",
    "covid_keywords = ['COVID', 'COVID-19', 'covid', 'pandemic', 'Pandemic', 'virus', 'Omicron', 'omicron', 'Delta', 'delta', 'variant', 'outbreak', 'mask', 'N95', 'KN95', 'wave', 'symptoms', 'testing', 'rapid test', 'pcr', 'PCR', 'social distancing', 'Social distancing', 'Social Distancing', 'epidemic', 'Epidemic', 'fatality rate', 'Fatality rate', 'Fatality Rate', 'flattening the curve', 'Flattening the Curve']\n",
    "\n",
    "where_to_look = ['div', 'section', 'span']\n",
    "\n",
    "dysfunctional_pages = ['ieee.org', 'www.naturalawakeningsmag.com', 'libertyvideos.org']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __crawler(url: str) -> None:\n",
    "    \"\"\"Crawls through MBFC provided links and stores their  content\n",
    "\n",
    "    :param url: an MBFC link to a list of pages under a specific genre\n",
    "    :type url: str\n",
    "    \"\"\"\n",
    "\n",
    "    # access the MBFC page linked via the url parameter\n",
    "    html_page = requests.get(url)\n",
    "\n",
    "    # parse through that particular page and find all of the news sites\n",
    "    soup = BeautifulSoup(html_page.content, 'lxml')\n",
    "    news_sites = soup.find_all('span', {'style': 'font-size: 12pt;'})\n",
    "\n",
    "    # creating storage for all of the links to be obtained off the MBFC page\n",
    "    webpages = []\n",
    "\n",
    "    # loop through the links found\n",
    "    for news_channel in news_sites:\n",
    "        # parse through list item and extract just the link\n",
    "        link = news_channel.text[news_channel.text.rfind('(')+1:-1]\n",
    "\n",
    "        # adjust the link based on if the 'https' prefix is present\n",
    "        if link[-4:] in suffixes:\n",
    "\n",
    "            # add the modified link for storage\n",
    "            if (link[:8] == 'https://'):\n",
    "                webpages.append(link[8:])\n",
    "            else:\n",
    "                webpages.append(link)\n",
    "\n",
    "    # delete scraped content if it exists currently\n",
    "    # if 'news_channels' in os.listdir():\n",
    "    #     shutil.rmtree('news_channels/')\n",
    "\n",
    "    # create a new directory to store the scraped content\n",
    "    os.mkdir('news_channels')\n",
    "\n",
    "    # loop through the webpages and access their\n",
    "    for website in webpages:\n",
    "\n",
    "        # if the website is a dysfunctional one, then skip over the current iteration\n",
    "        if website in dysfunctional_pages:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # access the current website's HTML and save it to a variable\n",
    "            current_html = str(requests.get('https://' + website).content)\n",
    "\n",
    "            # creating a new file in the current directory and saving the HTML accessed earlier to this file\n",
    "            with open('news_channels/' + website + 'html_page.txt', 'w') as rn:\n",
    "                rn.write(current_html)\n",
    "\n",
    "        except Exception:\n",
    "            # if the html cannot be obtained, then continue and skip over the current iteration\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __finder() -> list:\n",
    "    \"\"\"Finds articles when given\n",
    "\n",
    "    :return: a list of article objects (dict)\n",
    "    :rtype: list\n",
    "    \"\"\"\n",
    "    # create a variable to store the base path to avoid subsequent duplication \n",
    "    base_path = \"news_channels/\"\n",
    "\n",
    "    # locate all of the files under the base directory, i.e. the parsed news sites found from the url provided to MBFC\n",
    "    structure = os.listdir(base_path)\n",
    "\n",
    "    # creating a variable to store the articles found\n",
    "    overall = []\n",
    "\n",
    "    # loop through the files located under the base directory\n",
    "    for file in structure:\n",
    "        # using a combination of a relative and absolute path, determine the exact path to the current file\n",
    "        current_path = base_path + file\n",
    "\n",
    "        # open the current file to read its contents\n",
    "        with open(current_path, 'r') as current_soup:\n",
    "\n",
    "            # parsing the current file's HTML contents\n",
    "            soup = BeautifulSoup(current_soup.read(), 'lxml')\n",
    "\n",
    "            # creating a variable to store potential articles, to be added to the overall list after validation\n",
    "            potential_articles = []\n",
    "\n",
    "            # looping over all possible locations where the articles can occur\n",
    "            for i in range(3):\n",
    "                # applying a two-step check to find news articles\n",
    "                p1 = soup.find_all(where_to_look[i])\n",
    "                p2 = []\n",
    "\n",
    "                # looping over the occurences of the HTML element\n",
    "                for potential in p1:\n",
    "                    # applying a condition to determine eligibility\n",
    "                    if potential.has_attr('class'):\n",
    "                        p2.append(potential)\n",
    "\n",
    "                # looping over tags of screened HTML elements from the artiles\n",
    "                for tag in p2:\n",
    "                    # looping over those elements with an anchor attribute\n",
    "                    for anchor in tag.find_all('a'):\n",
    "                        # applying a condition to determine eligibility\n",
    "                        if not anchor.has_attr('href'):\n",
    "                            continue\n",
    "                        potential_articles.append(anchor)\n",
    "\n",
    "            # using a set() to remove duplicate article entries\n",
    "            potential_articles = list(set(potential_articles))\n",
    "\n",
    "            # validating whether approved articles are related to COVID-19 or not\n",
    "            covid_related = False\n",
    "            for article_title in potential_articles:\n",
    "\n",
    "                # cleaning up article text\n",
    "                mod_title = article_title.text\n",
    "                mod_title = ' '.join(mod_title.split())\n",
    "\n",
    "                # further screening checks to vet out any CSS\n",
    "                if 'css' in mod_title:\n",
    "                    continue\n",
    "\n",
    "                # checking if the current article is related to COVID-19 or not based on COVID-19 keywords defined above\n",
    "                for covid_word in covid_keywords:\n",
    "                    if covid_word in mod_title:\n",
    "                        covid_related = True\n",
    "\n",
    "                # adding the current article to the list of articles if it is COVID-19 related\n",
    "                if covid_related:\n",
    "\n",
    "                    # accessing the current title\n",
    "                    intended_link = article_title['href']\n",
    "\n",
    "                    # applying case-wise modifications to tidy up the url\n",
    "                    if intended_link[0] == '/' or intended_link[0] not in ['h', 'w']:\n",
    "                        intended_link = file[:-13] + intended_link\n",
    "                    if intended_link.count('http://') + intended_link.count('https://') == 0:\n",
    "                        intended_link = 'https://' + intended_link\n",
    "\n",
    "                    # creating a dictionary object to store the current article's parts\n",
    "                    article = {\n",
    "                        'title': mod_title,\n",
    "                        'link': intended_link,\n",
    "                    }\n",
    "\n",
    "                    # adding the current article to storage\n",
    "                    overall.append(article)\n",
    "\n",
    "                    # resetting parameters to be checked\n",
    "                    covid_related = False\n",
    "\n",
    "    return overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_articles(url: str) -> list:\n",
    "    \"\"\"Finds articles based on the MBFC link provided\n",
    "\n",
    "    :param url: an MBFC link to a list of pages under a specific genre\n",
    "    :type url: str\n",
    "    :return: a list of article objects (dict)\n",
    "    :rtype: list\n",
    "    \"\"\"\n",
    "    \n",
    "    # calling crawler and finder to obtain\n",
    "    __crawler(url)\n",
    "    articles = __finder()\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining two variables to store the MBFC page links for scientific and conspiracy articles\n",
    "science_url = 'https://mediabiasfactcheck.com/pro-science/'\n",
    "conspiracy_url = 'https://mediabiasfactcheck.com/conspiracy/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding the scientific articles\n",
    "science_articles = find_articles(science_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding the conspiracy articles\n",
    "conspiracy_articles = find_articles(conspiracy_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Saving JSON results obtained from Scraping</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# creating a function to store JSON files\n",
    "def write_to_storage(name: str, articles: list[str]) -> None:\n",
    "    # specifying a filename where to create a new file\n",
    "    filename = f\"../Data/extract/{name}.json\"\n",
    "\n",
    "    # creating a new file located at filename and writing JSON-ified articles into that file\n",
    "    with open(filename, 'w') as storage:\n",
    "        storage.write(json.dumps(articles, indent = 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing the scientific and conspiracy articles found to storage for further use\n",
    "\n",
    "write_to_storage('science', science_articles)\n",
    "write_to_storage('conspiracy', conspiracy_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# deleting the directory containing the articles as they are no longer needed\n",
    "shutil.rmtree('news_channels/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aa11b94746fc3e31dd444f69cbc07362dc8f0f9fc8892b4755141b10c2d19365"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
