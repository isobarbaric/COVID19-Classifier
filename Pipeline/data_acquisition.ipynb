{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import shutil\n",
    "from bs4 import BeautifulSoup\n",
    "from newspaper import Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffixes = ['.com', '.org', '.edu', '.gov', '.int', '.co', '.net', '.au', '.us', '.uk', '.ne', 'news'] \n",
    "    \n",
    "covid_keywords = ['COVID', 'COVID-19', 'covid', 'pandemic', 'Pandemic', 'virus', 'Omicron', 'omicron', 'Delta', 'delta', 'variant', 'outbreak', 'mask', 'N95', 'KN95', 'wave', 'symptoms', 'testing', 'rapid test', 'pcr', 'PCR', 'social distancing', 'Social distancing', 'Social Distancing', 'epidemic', 'Epidemic', 'fatality rate', 'Fatality rate', 'Fatality Rate', 'flattening the curve', 'Flattening the Curve']\n",
    "\n",
    "where_to_look = ['div', 'section', 'span']\n",
    "\n",
    "dysfunctional_pages = ['ieee.org', 'www.naturalawakeningsmag.com', 'libertyvideos.org']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __crawler(url: str) -> None:\n",
    "    html_page = requests.get(url) \n",
    "    soup = BeautifulSoup(html_page.content, 'lxml') \n",
    "    news_sites = soup.find_all('span', {'style': 'font-size: 12pt;'}) \n",
    "    webpages = [] \n",
    "    for news_channel in news_sites: \n",
    "        link = news_channel.text[news_channel.text.rfind('(')+1:-1]\n",
    "        if link[-4:] in suffixes:\n",
    "            if (link[:8] == 'https://'):\n",
    "                webpages.append(link[8:])\n",
    "            else:\n",
    "                webpages.append(link)\n",
    "    \n",
    "    if 'news_channels' in os.listdir():\n",
    "        shutil.rmtree('news_channels/')\n",
    "\n",
    "    os.mkdir('news_channels') \n",
    "\n",
    "    for website in webpages:\n",
    "        if website in dysfunctional_pages:\n",
    "            continue\n",
    "        try:\n",
    "            current_html = str(requests.get('https://' + website).content) \n",
    "            with open('news_channels/' + website + 'html_page.txt', 'w') as rn: \n",
    "                rn.write(current_html)\n",
    "        except Exception:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __finder(url: str) -> list:\n",
    "    base_path = \"news_channels/\"\n",
    "    structure = os.listdir('news_channels/')\n",
    "    overall = []\n",
    "\n",
    "    for file in structure:\n",
    "        current_path = base_path + file\n",
    "\n",
    "        with open(current_path, 'r') as current_soup:\n",
    "            soup = BeautifulSoup(current_soup.read(), 'lxml')\n",
    "\n",
    "            potential_articles = []\n",
    "\n",
    "            for i in range(3):\n",
    "                p1 = soup.find_all(where_to_look[i])\n",
    "                p2 = []\n",
    "\n",
    "                for potential in p1:\n",
    "                    if potential.has_attr('class'):\n",
    "                        p2.append(potential)\n",
    "\n",
    "                for tag in p2:\n",
    "                    for anchor in tag.find_all('a'):\n",
    "                        if not anchor.has_attr('href'):\n",
    "                            continue\n",
    "                        potential_articles.append(anchor)\n",
    "            potential_articles = list(set(potential_articles))\n",
    "\n",
    "            covid_related = False\n",
    "            for article_title in potential_articles:\n",
    "                mod_title = article_title.text\n",
    "                mod_title = ' '.join(mod_title.split())\n",
    "\n",
    "                if 'css' in mod_title:\n",
    "                    continue\n",
    "\n",
    "                for covid_word in covid_keywords:\n",
    "                    if covid_word in mod_title:\n",
    "                        covid_related = True\n",
    "\n",
    "                if covid_related:\n",
    "\n",
    "                    intended_link = article_title['href']\n",
    "\n",
    "                    # if intended_link[0] not in ['h', 'w'] and intended_link != '/':\n",
    "                    #     intended_link = '/' + intended_link\n",
    "\n",
    "                    if intended_link[0] == '/' or intended_link[0] not in ['h', 'w']:\n",
    "                        intended_link = file[:-13] + intended_link\n",
    "\n",
    "                    if intended_link.count('http://') + intended_link.count('https://') == 0:\n",
    "                        intended_link = 'https://' + intended_link\n",
    "\n",
    "                    article = {\n",
    "                        'title': mod_title,\n",
    "                        'link': intended_link,\n",
    "                    }\n",
    "\n",
    "                    overall.append(article)\n",
    "\n",
    "                    covid_related = False\n",
    "\n",
    "    return overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_articles(url: str) -> list:\n",
    "    __crawler(url)\n",
    "    return __finder(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "science_url = 'https://mediabiasfactcheck.com/pro-science/'\n",
    "conspiracy_url = 'https://mediabiasfactcheck.com/conspiracy/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "science_articles = find_articles(science_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "conspiracy_articles = find_articles(conspiracy_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def write_to_storage(name: str, articles: list[str]) -> None:\n",
    "    filename = f\"../Data/extract/{name}.json\"\n",
    "    with open(filename, 'w') as storage:\n",
    "        storage.write(json.dumps(articles, indent = 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_storage('science', science_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_storage('conspiracy', conspiracy_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.rmtree('news_channels/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aa11b94746fc3e31dd444f69cbc07362dc8f0f9fc8892b4755141b10c2d19365"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
